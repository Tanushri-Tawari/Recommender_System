{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Deep Matrix Factorization Method (DMF) on \"Book-Crossing Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the Book-Crossing dataset\n",
    "path_to_data = 'data/BX-Book-Ratings.csv'\n",
    "data = pd.read_csv(path_to_data, encoding='latin-1', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# For simplicity, let's consider only users who have rated at least 5 books\n",
    "user_counts = data['User-ID'].value_counts()\n",
    "active_users = user_counts[user_counts >= 5].index\n",
    "filtered_data = data[data['User-ID'].isin(active_users)]\n",
    "\n",
    "# Encode user and item IDs\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "filtered_data['user_id'] = user_encoder.fit_transform(filtered_data['User-ID'])\n",
    "filtered_data['item_id'] = item_encoder.fit_transform(filtered_data['ISBN'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(filtered_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep matrix factorization model using Keras\n",
    "class DeepMatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, latent_dim):\n",
    "        super(DeepMatrixFactorization, self).__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, latent_dim)\n",
    "        self.item_embedding = tf.keras.layers.Embedding(num_items, latent_dim)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs\n",
    "        user_embedding = self.flatten(self.user_embedding(user_input))\n",
    "        item_embedding = self.flatten(self.item_embedding(item_input))\n",
    "        concatenated = tf.concat([user_embedding, item_embedding], axis=-1)\n",
    "        x = self.dense1(concatenated)\n",
    "        output = self.dense2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a basic example.\n",
    "# We may need to adjust the model architecture and hyperparameters based on the specific needs and dataset characteristics. \n",
    "# Additionally, we can consider using more advanced techniques, such as incorporating additional features or using more sophisticated architectures, for better performance in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique users and items\n",
    "num_users = len(filtered_data['user_id'].unique())\n",
    "num_items = len(filtered_data['item_id'].unique())\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 50\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Instantiate the model\n",
    "model = DeepMatrixFactorization(num_users, num_items, latent_dim)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training data\n",
    "train_user_ids = train_data['user_id'].values\n",
    "train_item_ids = train_data['item_id'].values\n",
    "train_labels = np.ones_like(train_user_ids)  # Since we are using binary cross-entropy loss\n",
    "\n",
    "# Test data\n",
    "test_user_ids = test_data['user_id'].values\n",
    "test_item_ids = test_data['item_id'].values\n",
    "test_labels = np.ones_like(test_user_ids)  # Since we are using binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit([train_user_ids, train_item_ids], train_labels, epochs=num_epochs, batch_size=batch_size, validation_data=([test_user_ids, test_item_ids], test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate([test_user_ids, test_item_ids], test_labels)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
